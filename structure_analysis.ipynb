{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr, ttest_rel, f_oneway\n",
    "from scipy.cluster import hierarchy\n",
    "from os import listdir, path\n",
    "from utils import create_attribute_dict, create_model_association_df, cohens_d, compute_normalized_frobenius_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path constants\n",
    "OMI_PATH = 'omi/attribute_means.csv'\n",
    "\n",
    "ATTRIBUTE_PATH = 'prompts'\n",
    "MODEL_IMPRESSIONS_PATH = 'first_impression_similarities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in OMI attribute rating data\n",
    "omi_ratings = pd.read_csv(OMI_PATH, index_col=0)\n",
    "\n",
    "# Get a list of the 34 OMI attributes\n",
    "omi_attributes = omi_ratings.columns.to_list()\n",
    "\n",
    "# Compute the human-human correlations and significance\n",
    "omi_correlations = [[pearsonr(omi_ratings[attribute], omi_ratings[attribute_])[0] for attribute in omi_attributes] for attribute_ in omi_attributes]\n",
    "\n",
    "# Create dataframes of the human-human correlation data\n",
    "omi_correlation_df = pd.DataFrame(omi_correlations, columns=omi_attributes, index=omi_attributes)\n",
    "omi_correlation_df.index.name = 'Attribute'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each attribute to its positive polar prompt\n",
    "attribute_dict = create_attribute_dict(path.join(ATTRIBUTE_PATH,'attributes.txt'))\n",
    "\n",
    "# Create a dictionary mapping each attribute to its opposite prompt (the prompt for the opposing pole of the attribute)\n",
    "opposite_dict = create_attribute_dict(path.join(ATTRIBUTE_PATH,'attributes_opposites.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the models with similarity data saved as pickles\n",
    "model_pickles = [i for i in listdir(MODEL_IMPRESSIONS_PATH) if i.split('.')[-1] == 'pkl']\n",
    "\n",
    "# Get a list of the model names from the pickle file names\n",
    "model_names = [i.split('_first_impression_similarities.pkl')[0] for i in model_pickles]\n",
    "\n",
    "# Create a dictionary mapping model names to their similarity data\n",
    "models_dict = dict(zip(model_names, model_pickles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dict to store model-model correlation dataframes\n",
    "model_correlation_dfs, frob_products = {}, {}\n",
    "\n",
    "# Iterate through each model\n",
    "for model_name, model_file in models_dict.items():\n",
    "\n",
    "    # Read in the model similarity data\n",
    "    with open(path.join(MODEL_IMPRESSIONS_PATH, model_file), 'rb') as f:\n",
    "        model_similarity_dict = pkl.load(f)\n",
    "    \n",
    "    # Create a dataframe of the model similarity data\n",
    "    model_similarity_df = pd.DataFrame(model_similarity_dict)\n",
    "\n",
    "    # Create a dataframe of the difference between the cosine similarity of each image to the positive prompt and the negative prompt for each attribute in a model\n",
    "    model_association_df = create_model_association_df(model_similarity_df, attribute_dict, opposite_dict, baseline='difference')\n",
    "\n",
    "    # Compute the model-model correlations and significance\n",
    "    model_correlations = [[pearsonr(model_association_df[attribute], model_association_df[attribute_])[0] for attribute in omi_attributes] for attribute_ in omi_attributes]\n",
    "        \n",
    "    # Create dataframes of the model-model correlation data\n",
    "    model_correlation_df = pd.DataFrame(model_correlations, columns=omi_attributes, index=omi_attributes)\n",
    "    model_correlation_df.index.name = 'Attribute'\n",
    "\n",
    "    # Compute the normalized Frobenius inner product between the model-model and human-human correlation matrices\n",
    "    normalized_frob_inner_product = compute_normalized_frobenius_product(omi_correlation_df.to_numpy(), model_correlation_df.to_numpy())\n",
    "\n",
    "    # Add the model correlation dataframe and the normalized Frobenius inner product to dictionaries\n",
    "    model_correlation_dfs[model_name] = model_correlation_df\n",
    "    frob_products[model_name] = normalized_frob_inner_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Frobenius products by model family\n",
    "models2b, models400m, models80m = [], [], []\n",
    "\n",
    "for model_name in frob_products.keys():\n",
    "    if '2B' in model_name and 'g-14' not in model_name and 'H-14' not in model_name: # Exclude g-14 and H-14 models\n",
    "        models2b.append(frob_products[model_name])\n",
    "    elif '400M' in model_name:\n",
    "        models400m.append(frob_products[model_name])\n",
    "    elif '80M' in model_name:\n",
    "        models80m.append(frob_products[model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get boxplot data for 80m, 400m, and 2b models\n",
    "\n",
    "min_80m, quartile_80m, median_80m, quartile3_80m, max_80m = min(models80m), np.percentile(models80m, 25), np.percentile(models80m, 50), np.percentile(models80m, 75), max(models80m)\n",
    "min_400m, quartile_400m, median_400m, quartile3_400m, max_400m = min(models400m), np.percentile(models400m, 25), np.percentile(models400m, 50), np.percentile(models400m, 75), max(models400m)\n",
    "min_2b, quartile_2b, median_2b, quartile3_2b, max_2b = min(models2b), np.percentile(models2b, 25), np.percentile(models2b, 50), np.percentile(models2b, 75), max(models2b)\n",
    "\n",
    "print(f'lower whisker={min_80m}, lower quartile={quartile_80m}, median={median_80m}, upper quartile={quartile3_80m}, upper whisker={max_80m}')\n",
    "print(f'lower whisker={min_400m}, lower quartile={quartile_400m}, median={median_400m}, upper quartile={quartile3_400m}, upper whisker={max_400m}')\n",
    "print(f'lower whisker={min_2b}, lower quartile={quartile_2b}, median={median_2b}, upper quartile={quartile3_2b}, upper whisker={max_2b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of model-model correlation data by dataset size, approximating the visualization in the paper\n",
    "sns.boxplot(data=[models2b, models400m, models80m], showfliers=False)\n",
    "plt.xticks([0,1,2], ['Scaling-2b', 'Scaling-400m', 'Scaling-80m'], fontname='Times New Roman', fontsize=12, rotation=15, ha='center')\n",
    "plt.yticks(fontname='Times New Roman', fontsize=12)\n",
    "plt.ylabel('Normalized Inner Product', fontname='Times New Roman', fontsize=14)\n",
    "plt.xlabel('Dataset', fontname='Times New Roman', fontsize=14)\n",
    "plt.title('Similarity of Model and Human Correlations by Dataset Size', fontsize=14, fontname='Times New Roman')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA of model-model correlation data by dataset size\n",
    "print(f_oneway(models2b, models400m, models80m))\n",
    "\n",
    "# Correct p-value for multiple comparisons\n",
    "print(f_oneway(models2b, models400m, models80m)[1]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired t-tests of model-model correlation data vs. human-human correlation data by dataset size\n",
    "print('2b-400m')\n",
    "print(ttest_rel(models2b, models400m))\n",
    "print('2b-80m')\n",
    "print(ttest_rel(models2b, models80m))\n",
    "print('400m-80m')\n",
    "print(ttest_rel(models400m, models80m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cohen's d for each paired t-test\n",
    "print('2b-400m')\n",
    "print(cohens_d(models2b, models400m))\n",
    "print('2b-80m')\n",
    "print(cohens_d(models2b, models80m))\n",
    "print('400m-80m')\n",
    "print(cohens_d(models400m, models80m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model for further analysis\n",
    "corr_df = model_correlation_dfs['scaling_ViT-L-14__Model-L-14_Data-2B_Samples-13B_lr-1e-3_bs-86k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the human and model-model correlation dendrograms as subfigures in a figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(4.2, 10))\n",
    "fig.suptitle('Human and Model Attribute Correlation Clustering', fontsize=16, fontname='Times New Roman', y=0.93)\n",
    "\n",
    "# Plot the human-human correlation dendrogram\n",
    "i = hierarchy.linkage(omi_correlation_df.corr(), method='ward')\n",
    "dn = hierarchy.dendrogram(i, labels=corr_df.columns, leaf_font_size=12, leaf_rotation=0, ax=ax1, orientation='right')\n",
    "ax1.set_title('Human (OMI Dataset)', fontsize=15, fontname='Times New Roman')\n",
    "plt.setp(ax1.get_yticklabels(), fontname='Times New Roman', size=14)\n",
    "plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "ax1.set_xticks([])\n",
    "\n",
    "# Plot the model-model correlation dendrogram\n",
    "k = hierarchy.linkage(corr_df, method='ward')\n",
    "dn = hierarchy.dendrogram(k, labels=corr_df.columns, leaf_font_size=12, leaf_rotation=0,  ax=ax2, orientation='left')\n",
    "ax2.set_title('CLIP-ViT-L-14', fontsize=15, fontname='Times New Roman')\n",
    "plt.setp(ax2.get_yticklabels(), fontname='Times New Roman', size=14)\n",
    "plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "ax2.set_xticks([])\n",
    "\n",
    "current_plot = plt.gcf()\n",
    "plt.xticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap with dendrogram of model-model correlation data for CLIP-ViT-L-14\n",
    "g = sns.clustermap(corr_df, cmap='coolwarm', row_cluster=True, col_cluster=True, figsize=(10,10), cbar_kws={'label': 'Pearson\\'s r'}, vmin=-1, vmax=1,\n",
    "               row_linkage=hierarchy.linkage(corr_df, method='ward'), col_linkage=hierarchy.linkage(corr_df, method='ward'),\n",
    "               xticklabels=corr_df.columns, yticklabels=corr_df.columns, cbar_pos=(0.0, 0.87, 0.03, 0.1),\n",
    "               dendrogram_ratio=(.1, .1), edgecolor='black', linewidths=.5)\n",
    "\n",
    "# Set plot labels\n",
    "plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), fontname='Times New Roman', size=14)\n",
    "plt.setp(g.ax_heatmap.xaxis.get_majorticklabels(), fontname='Times New Roman', size=14)\n",
    "plt.setp(g.ax_heatmap.yaxis.get_label(), fontname='Times New Roman', size=16)\n",
    "plt.setp(g.ax_cbar.yaxis.get_majorticklabels(), fontname='Times New Roman', size=12)\n",
    "plt.setp(g.ax_cbar.yaxis.get_label(), fontname='Times New Roman', size=12)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap with dendrogram of human correlation data from OMI\n",
    "g = sns.clustermap(omi_correlation_df, cmap='coolwarm', row_cluster=True, col_cluster=True, figsize=(10,10), cbar_kws={'label': 'Pearson\\'s r'}, vmin=-1, vmax=1,\n",
    "               row_linkage=hierarchy.linkage(omi_correlation_df, method='ward'), col_linkage=hierarchy.linkage(omi_correlation_df, method='ward'),\n",
    "               xticklabels=omi_correlation_df.columns, yticklabels=omi_correlation_df.columns, cbar_pos=(0.0, 0.87, 0.03, 0.1),\n",
    "               dendrogram_ratio=(.1, .1), edgecolor='black', linewidths=.5)\n",
    "\n",
    "# Set plot labels\n",
    "plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), fontname='Times New Roman', size=14)\n",
    "plt.setp(g.ax_heatmap.xaxis.get_majorticklabels(), fontname='Times New Roman', size=14)\n",
    "plt.setp(g.ax_heatmap.yaxis.get_label(), fontname='Times New Roman', size=16)\n",
    "plt.setp(g.ax_cbar.yaxis.get_majorticklabels(), fontname='Times New Roman', size=12)\n",
    "plt.setp(g.ax_cbar.yaxis.get_label(), fontname='Times New Roman', size=12)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
