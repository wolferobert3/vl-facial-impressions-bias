{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import spearmanr, ttest_rel\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path constants\n",
    "OMI_PATH = 'omi/attribute_means.csv'\n",
    "HUMAN_IRR_PATH = 'peterson_irr/human_irr.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in human IRR data to a dictionary\n",
    "human_irr = pd.read_csv(HUMAN_IRR_PATH)\n",
    "human_irr_dict = {row['attribute']: row['human_irr'] for index, row in human_irr.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in OMI attribute rating data\n",
    "omi_ratings = pd.read_csv(OMI_PATH, index_col=0)\n",
    "\n",
    "# Get a list of the 34 OMI attributes\n",
    "omi_attributes = omi_ratings.columns.to_list()\n",
    "\n",
    "# View the first 5 rows of the data\n",
    "omi_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ViT-embedded OMI attribute vectors\n",
    "vecs = np.load('./text_to_image/vit_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true: np.ndarray, \n",
    "             y_pred: np.ndarray) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate the F1 score of a model's predictions and return precision and recall.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure that the true and predicted labels are comparable\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    tp = np.sum(y_true * y_pred)\n",
    "    fp = np.sum((1 - y_true) * y_pred)\n",
    "    fn = np.sum(y_true * (1 - y_pred))\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    # Return F1 score, precision, and recall\n",
    "    return f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_dict = {}\n",
    "param_dict = {}\n",
    "\n",
    "# Iterate over each OMI attribute, obtaining weight vectors and calculating F1 scores based on projections\n",
    "for attribute in omi_attributes:\n",
    "    \n",
    "    y = omi_ratings[attribute]\n",
    "    X = np.array(vecs)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    params = np.array(model.params)\n",
    "    param_dict[attribute] = params\n",
    "\n",
    "    tti_vecs = np.load(f'./text_to_image/{attribute}_sdxl_vit_vecs.npy')\n",
    "    projection = np.dot(tti_vecs, params)\n",
    "    projection = projection / np.linalg.norm(params)\n",
    "\n",
    "    pred = [1 if i > 0 else 0 for i in projection]\n",
    "    ground_truth = [1 for i in range(25)] + [0 for i in range(25)]\n",
    "    f1, precision, recall = f1_score(np.array(ground_truth), np.array(pred))\n",
    "    f1_dict[attribute] = f1\n",
    "\n",
    "# Sort the F1 scores in descending order to identify the attributes most reflected in the model\n",
    "sorted_f1_attributes = sorted(f1_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table_string = ''\n",
    "f1_only_table_string = ''\n",
    "tti_models = ['sdxl', 'sd2', 'runwaysd']\n",
    "tti_f1_dict = {'sdxl': {}, 'sd2': {}, 'runwaysd': {}}\n",
    "\n",
    "# Compute F1 scores and write to a LaTeX table\n",
    "for i in sorted_f1_attributes:\n",
    "\n",
    "    attribute = i[0]\n",
    "    params = param_dict[attribute]\n",
    "\n",
    "    attribute_text_string = f'{attribute}'\n",
    "    attribute_f1_string = f'{attribute}'\n",
    "\n",
    "    # Iterate over each TTI model to calculate F1 scores\n",
    "    for idx, tti_model in enumerate(tti_models):\n",
    "        tti_vecs = np.load(f'./text_to_image/{attribute}_{tti_model}_vit_vecs.npy')\n",
    "        projection = np.dot(tti_vecs, params)\n",
    "        projection = projection / np.linalg.norm(params)\n",
    "\n",
    "        pred = [1 if i > 0 else 0 for i in projection]\n",
    "        ground_truth = [1 for i in range(25)] + [0 for i in range(25)]\n",
    "\n",
    "        f1, precision, recall = f1_score(np.array(ground_truth), np.array(pred))\n",
    "        tti_latex_string = f'\\gray{{{int(round(f1, 2)*100)}}}{round(f1,2)} & \\gray{{{int(round(precision, 2)*100)}}}{round(precision,2)} & \\gray{{{int(round(recall, 2)*100)}}}{round(recall,2)}'\n",
    "        tti_f1_string = f'\\gray{{{int(round(f1, 2)*100)}}}{round(f1,2)}'\n",
    "        attribute_text_string = attribute_text_string + ' & ' + tti_latex_string\n",
    "        attribute_f1_string = attribute_f1_string + ' & ' + tti_f1_string\n",
    "\n",
    "        tti_f1_dict[tti_model][attribute] = f1\n",
    "\n",
    "    latex_table_string = latex_table_string + attribute_text_string + ' \\\\\\\\ \\n'\n",
    "    f1_only_table_string = f1_only_table_string + attribute_f1_string + ' \\\\\\\\ \\n'\n",
    "\n",
    "# Write the LaTeX table to a file\n",
    "with open('./text_to_image/f1_scores.txt', 'w') as f:\n",
    "    f.write(latex_table_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Spearman correlation between human IRR and model F1 scores\n",
    "for tti_model, tti_f1_dict in tti_f1_dict.items():\n",
    "    \n",
    "    human_irr_list = [human_irr_dict[attribute]/100 for attribute, value in tti_f1_dict.items()]\n",
    "    model_f1_list = [value for attribute, value in tti_f1_dict.items()]\n",
    "\n",
    "    spearman = spearmanr(human_irr_list, model_f1_list)\n",
    "\n",
    "    print(f'{tti_model} Spearman: {spearman}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size_dict = {}\n",
    "\n",
    "# Take positive prompt vectors\n",
    "black_vecs = np.load(f'./text_to_image/black_sdxl_vit_vecs.npy')[:25]\n",
    "white_vecs = np.load(f'./text_to_image/white_sdxl_vit_vecs.npy')[:25]\n",
    "\n",
    "# Calculate effect sizes for each attribute\n",
    "for idx, att in enumerate(list(param_dict.keys())):\n",
    "\n",
    "    proj_black = np.dot(black_vecs, param_dict[att])\n",
    "    proj_black = proj_black / np.linalg.norm(param_dict[att])\n",
    "\n",
    "    proj_white = np.dot(white_vecs, param_dict[att])\n",
    "    proj_white = proj_white / np.linalg.norm(param_dict[att])\n",
    "\n",
    "    effect_size = (np.mean(proj_white) - np.mean(proj_black)) / (np.std(np.concatenate([proj_black, proj_white]), ddof=1))\n",
    "    effect_size_dict[att] = effect_size\n",
    "\n",
    "# Sort the effect sizes in descending order to order the plot below\n",
    "sorted_attributes = sorted(effect_size_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_code_list = []\n",
    "latex_vis_string = ''\n",
    "\n",
    "black_vecs = np.load(f'./text_to_image/black_sdxl_vit_vecs.npy')[:25]\n",
    "white_vecs = np.load(f'./text_to_image/white_sdxl_vit_vecs.npy')[:25]\n",
    "\n",
    "# Generate LaTeX code for visualizing effect sizes with boxplots\n",
    "for idx, att in enumerate(sorted_attributes):\n",
    "\n",
    "    proj_black = np.dot(black_vecs, param_dict[att[0]])\n",
    "    proj_black = proj_black / np.linalg.norm(param_dict[att[0]])\n",
    "    black_median, black_q1, black_q3 = np.percentile(proj_black, [50, 25, 75])\n",
    "    # Write boxplot for distribution of projections\n",
    "    black_print = f'\\\\addplot+[boxplot prepared={{lower whisker={black_q1}, lower quartile={black_q1}, median={black_median}, upper quartile={black_q3}, upper whisker={black_q3}}}, color=black, fill=red!65, solid] coordinates {{}};'\n",
    "\n",
    "    proj_white = np.dot(white_vecs, param_dict[att[0]])\n",
    "    proj_white = proj_white / np.linalg.norm(param_dict[att[0]])\n",
    "    white_median, white_q1, white_q3 = np.percentile(proj_white, [50, 25, 75])\n",
    "    # Write boxplot for distribution of projections\n",
    "    white_print = f'\\\\addplot+[boxplot prepared={{lower whisker={white_q1}, lower quartile={white_q1}, median={white_median}, upper quartile={white_q3}, upper whisker={white_q3}}}, color=black, fill=blue!65, solid] coordinates {{}};'\n",
    "\n",
    "    latex_vis_string = latex_vis_string + white_print + '\\n'\n",
    "    latex_vis_string = latex_vis_string + black_print + '\\n'\n",
    "\n",
    "    # Perform a paired t-test to determine significance of the difference between the two groups\n",
    "    t_test = ttest_rel(proj_black, proj_white).pvalue\n",
    "    sig_asterisks = f'\\\\textit{{n.s.}}'\n",
    "    if t_test < 0.001:\n",
    "        sig_asterisks = '***'\n",
    "    elif t_test < 0.01:\n",
    "        sig_asterisks = '**'\n",
    "    elif t_test < 0.05:\n",
    "        sig_asterisks = '*'\n",
    "\n",
    "    # Get the effect size for the attribute and round to two decimal places\n",
    "    effect_size = str(round(att[1], 2))\n",
    "\n",
    "    draw_idx = 1 if idx == 0 else (idx*2)+1\n",
    "    \n",
    "    # Write LaTeX code for drawing a brace to the effect size and significance level\n",
    "    drawing_code = f\"\"\"\n",
    "    \\draw [thick] (axis cs:{draw_idx},-10) -- (axis cs:{draw_idx},{white_q1});\n",
    "    \\draw [thick] (axis cs:{draw_idx+1},-10) -- (axis cs:{draw_idx+1},{black_q1});\n",
    "    \\draw [thick] (axis cs:{draw_idx},-10) -- (axis cs:{draw_idx+1},-10,);\n",
    "    \\\\node at (axis cs:{draw_idx}.6,-11) {{\\\\tiny {sig_asterisks}}};\n",
    "    \\\\node at (axis cs:{draw_idx}.6,-12) {{\\\\tiny {effect_size}}};\n",
    "    \"\"\"\n",
    "\n",
    "    draw_code_list.append(drawing_code)\n",
    "\n",
    "draw_codes = '\\n'.join(draw_code_list)\n",
    "latex_vis_string = latex_vis_string + '\\n' + draw_codes\n",
    "\n",
    "# Write the LaTeX code to a file\n",
    "with open('./text_to_image/effect_size_vis.txt', 'w') as f:\n",
    "    f.write(latex_vis_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the xticks for the attribute labels\n",
    "xticks = '.7, '.join([str(i+1) for i in range(len(sorted_attributes)*2)])\n",
    "print(xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the xticks with the attribute names\n",
    "xtick_labels = ', , '.join([att[0] for att in sorted_attributes])\n",
    "print(xtick_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the effect sizes for each attribute\n",
    "for attribute, effect in effect_size_dict.items():\n",
    "    print(f'{attribute}: {effect}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
