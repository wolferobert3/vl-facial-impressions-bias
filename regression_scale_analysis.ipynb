{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from os import listdir, path\n",
    "from utils import create_attribute_dict, create_model_association_df, create_model_human_similarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path constants\n",
    "OMI_PATH = 'omi/attribute_means.csv'\n",
    "SCALING_INFO = 'scaling_info/trained_models_info.csv'\n",
    "ARCH_INFO = 'scaling_info/arch_info.csv'\n",
    "HUMAN_IRR_PATH = 'peterson_irr/human_irr.csv'\n",
    "\n",
    "ATTRIBUTE_PATH = 'prompts'\n",
    "MODEL_IMPRESSIONS_PATH = 'first_impression_similarities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in OMI attribute rating data\n",
    "omi_ratings = pd.read_csv(OMI_PATH, index_col=0)\n",
    "\n",
    "# Get a list of the 34 OMI attributes\n",
    "omi_attributes = omi_ratings.columns.to_list()\n",
    "\n",
    "# View the first 5 rows of the data\n",
    "omi_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each attribute to its positive polar prompt\n",
    "attribute_dict = create_attribute_dict(path.join(ATTRIBUTE_PATH,'attributes.txt'))\n",
    "\n",
    "# Create a dictionary mapping each attribute to its opposite prompt (the prompt for the opposing pole of the attribute)\n",
    "opposite_dict = create_attribute_dict(path.join(ATTRIBUTE_PATH,'attributes_opposites.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the models with similarity data saved as pickles\n",
    "model_pickles = [i for i in listdir(MODEL_IMPRESSIONS_PATH) if i.split('.')[-1] == 'pkl' and i.split('_')[0] == 'scaling']\n",
    "model_pickles = [i for i in model_pickles if 'g-14' not in i and 'H-14' not in i]\n",
    "\n",
    "# Get a list of the model names from the pickle file names\n",
    "model_names = [i.split('_first_impression_similarities.pkl')[0] for i in model_pickles]\n",
    "\n",
    "# Create a dictionary mapping model names to their similarity data\n",
    "models_dict = dict(zip(model_names, model_pickles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store model-human correlation and statistical significance dataframes\n",
    "model_human_dfs, model_human_sig_dfs = [], []\n",
    "\n",
    "# Iterate through each model\n",
    "for model_name, model_file in models_dict.items():\n",
    "\n",
    "    # Read in the model similarity data\n",
    "    with open(path.join(MODEL_IMPRESSIONS_PATH, model_file), 'rb') as f:\n",
    "        model_similarity_dict = pkl.load(f)\n",
    "    \n",
    "    # Create a dataframe of the model similarity data\n",
    "    model_similarity_df = pd.DataFrame(model_similarity_dict)\n",
    "\n",
    "    # Create a dataframe of the difference between the cosine similarity of each image to the positive prompt and the negative prompt for each attribute in a model\n",
    "    model_association_df = create_model_association_df(model_similarity_df, attribute_dict, opposite_dict, baseline='difference')\n",
    "\n",
    "    # Create dictionaries of the Spearman's r correlation coefficient and significances between the model association and the OMI rating for each attribute in a model\n",
    "    model_human_correlations, model_human_sigs = create_model_human_similarity_dict(model_association_df, attribute_dict, omi_ratings)\n",
    "\n",
    "    # Create dataframes of the model-human correlations and significances\n",
    "    model_human_df, model_human_sig_df = pd.DataFrame(model_human_correlations, index=[model_name]), pd.DataFrame(model_human_sigs, index=[model_name])\n",
    "\n",
    "    # Append the model-human correlation and significance dataframes to lists\n",
    "    model_human_dfs.append(model_human_df)\n",
    "    model_human_sig_dfs.append(model_human_sig_df)\n",
    "\n",
    "# Concatenate model-human correlation and significance dataframes into single dataframes with model names as indices\n",
    "model_human_df = pd.concat(model_human_dfs)\n",
    "model_human_sig_df = pd.concat(model_human_sig_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data related to data scale and model architecture\n",
    "scaling_data = pd.read_csv(SCALING_INFO, index_col=0)\n",
    "arch_data = pd.read_csv(ARCH_INFO, index_col=0)\n",
    "human_irr = pd.read_csv(HUMAN_IRR_PATH, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map data size to number of unique examples (in millions)\n",
    "data_sizes = {'2B': 2320, '400M': 407, '80M': 80}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of scaling data\n",
    "scaling_regression_df = scaling_data[['samples_per_epoch', 'epochs', 'data']]\n",
    "\n",
    "# Transform the scaling data to be in terms of total samples\n",
    "scaling_regression_df['total_samples'] = scaling_regression_df['samples_per_epoch'] * scaling_regression_df['epochs']\n",
    "\n",
    "# Map data size to number of unique examples\n",
    "scaling_regression_df['data'] = scaling_regression_df['data'].map(data_sizes)\n",
    "\n",
    "# Obtain the number of parameters for each model from the architecture data\n",
    "scaling_regression_df['model'] = scaling_regression_df.index\n",
    "scaling_regression_df['image_mparams'] = scaling_regression_df['model'].apply(lambda x: arch_data.loc[x.split('_')[0].replace('Model', 'ViT'), 'image_mparams'])\n",
    "scaling_regression_df['text_mparams'] = scaling_regression_df['model'].apply(lambda x: arch_data.loc[x.split('_')[0].replace('Model', 'ViT'), 'text_mparams'])\n",
    "\n",
    "# Replace the model names with the model names without the pt checkpoint extension\n",
    "scaling_regression_df.index = [i.replace('.pt','') for i in scaling_regression_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the model-human correlation as dependent variable and data size, total samples, number of parameters, and attribute IRR as independent variables\n",
    "intermediate_dataframes = []\n",
    "attributes = model_human_df.columns.to_list()\n",
    "\n",
    "# Iterate through each model\n",
    "for model in model_human_df.index:\n",
    "\n",
    "    # Get model name as it appears in the scaling regression dataframe\n",
    "    loc_model = 'Model-' + model.split('Model-')[1]\n",
    "\n",
    "    # For each model, get the model-human correlation for each attribute\n",
    "    model_series = model_human_df.loc[model].to_list()\n",
    "\n",
    "    # Create a dataframe of that model's model-human correlations for the attributes\n",
    "    model_df = pd.DataFrame(model_series, columns=['mh_similarity'], index=attributes)\n",
    "\n",
    "    # Add the model's data size, total samples, and number of parameters to the dataframe\n",
    "    model_df['data'] = scaling_regression_df.loc[loc_model, 'data']\n",
    "    model_df['total_samples'] = scaling_regression_df.loc[loc_model, 'total_samples']\n",
    "    model_df['image_mparams'] = scaling_regression_df.loc[loc_model, 'image_mparams']\n",
    "    model_df['text_mparams'] = scaling_regression_df.loc[loc_model, 'text_mparams']\n",
    "\n",
    "    # Add the human inter-rater reliability for all of the attributes to the dataframe\n",
    "    model_df['human_irr'] = [float(human_irr.loc[attribute])/100. for attribute in model_df.index]\n",
    "\n",
    "    # Add the model name and attribute name as the index\n",
    "    model_df.index = [f'{model}_{attribute}' for attribute in model_df.index]\n",
    "\n",
    "    # Append the dataframe to a list\n",
    "    intermediate_dataframes.append(model_df)\n",
    "\n",
    "# Concatenate the dataframes into a single dataframe\n",
    "final_regression_df = pd.concat(intermediate_dataframes, axis=0)\n",
    "final_regression_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data by dividing by the maximum value for each column\n",
    "normalized_df = final_regression_df.copy()\n",
    "\n",
    "# Exclude columns with range in 0, 1 from normalization\n",
    "norm_columns = [column for column in normalized_df.columns if column not in ['human_irr', 'mh_similarity']]\n",
    "\n",
    "# Normalize columns\n",
    "for column in norm_columns:\n",
    "    normalized_df[column] = normalized_df[column] / normalized_df[column].max()\n",
    "\n",
    "normalized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a multiple linear regression model to predict model-human similarity from data size, total samples, number of parameters, and human inter-rater reliability\n",
    "\n",
    "# Dependent variable is the model-human similarity\n",
    "y = normalized_df['mh_similarity']\n",
    "\n",
    "# Independent variables are data size, total samples, number of parameters, and human inter-rater reliability\n",
    "X = normalized_df[['data', 'total_samples', 'image_mparams', 'text_mparams', 'human_irr']]\n",
    "\n",
    "# Add a constant to the independent variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
