{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir, path\n",
    "\n",
    "from utils import create_attribute_dict, create_model_association_df, create_model_human_similarity_dict, get_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path constants\n",
    "OMI_PATH = 'omi/attribute_means.csv'\n",
    "HUMAN_IRR_PATH = 'peterson_irr/human_irr.csv'\n",
    "\n",
    "ATTRIBUTE_PATH = 'prompts'\n",
    "MODEL_IMPRESSIONS_PATH = 'first_impression_similarities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in OMI attribute rating data\n",
    "omi_ratings = pd.read_csv(OMI_PATH, index_col=0)\n",
    "\n",
    "# Get a list of the 34 OMI attributes\n",
    "omi_attributes = omi_ratings.columns.to_list()\n",
    "\n",
    "# View the first 5 rows of the data\n",
    "omi_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each attribute to its positive polar prompt\n",
    "attribute_dict = create_attribute_dict(path.join(ATTRIBUTE_PATH,'attributes.txt'))\n",
    "\n",
    "# Create a dictionary mapping each attribute to its opposite prompt (the prompt for the opposing pole of the attribute)\n",
    "opposite_dict = create_attribute_dict(path.join(ATTRIBUTE_PATH,'attributes_opposites.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the models with similarity data saved as pickles\n",
    "model_pickles = [i for i in listdir(MODEL_IMPRESSIONS_PATH) if i.split('.')[-1] == 'pkl']\n",
    "\n",
    "# Get a list of the model names from the pickle file names\n",
    "model_names = [i.split('_first_impression_similarities.pkl')[0] for i in model_pickles]\n",
    "\n",
    "# Create a dictionary mapping model names to their similarity data\n",
    "models_dict = dict(zip(model_names, model_pickles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store model-human correlation and statistical significance dataframes\n",
    "model_human_dfs, model_human_sig_dfs = [], []\n",
    "\n",
    "# Iterate through each model\n",
    "for model_name, model_file in models_dict.items():\n",
    "\n",
    "    # Read in the model similarity data\n",
    "    with open(path.join(MODEL_IMPRESSIONS_PATH, model_file), 'rb') as f:\n",
    "        model_similarity_dict = pkl.load(f)\n",
    "    \n",
    "    # Create a dataframe of the model similarity data\n",
    "    model_similarity_df = pd.DataFrame(model_similarity_dict)\n",
    "\n",
    "    # Create a dataframe of the difference between the cosine similarity of each image to the positive prompt and the negative prompt for each attribute in a model\n",
    "    model_association_df = create_model_association_df(model_similarity_df, attribute_dict, opposite_dict, baseline='difference')\n",
    "\n",
    "    # Create dictionaries of the Spearman's r correlation coefficient and significances between the model association and the OMI rating for each attribute in a model\n",
    "    model_human_correlations, model_human_sigs = create_model_human_similarity_dict(model_association_df, attribute_dict, omi_ratings)\n",
    "\n",
    "    # Create dataframes of the model-human correlations and significances\n",
    "    model_human_df, model_human_sig_df = pd.DataFrame(model_human_correlations, index=[model_name]), pd.DataFrame(model_human_sigs, index=[model_name])\n",
    "\n",
    "    # Append the model-human correlation and significance dataframes to lists\n",
    "    model_human_dfs.append(model_human_df)\n",
    "    model_human_sig_dfs.append(model_human_sig_df)\n",
    "\n",
    "# Concatenate model-human correlation and significance dataframes into single dataframes with model names as indices\n",
    "model_human_df = pd.concat(model_human_dfs)\n",
    "model_human_sig_df = pd.concat(model_human_sig_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in human inter-rater reliability data provided in the body of Peterson et al. (2022)\n",
    "human_irr = pd.read_csv(HUMAN_IRR_PATH, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model-human correlations by model family\n",
    "openai_df = model_human_df.copy().loc[get_model_names(MODEL_IMPRESSIONS_PATH, 'openai')]\n",
    "scaling_df = model_human_df.copy().loc[get_model_names(MODEL_IMPRESSIONS_PATH, 'scaling')]\n",
    "faceclip_df = model_human_df.copy().loc[get_model_names(MODEL_IMPRESSIONS_PATH, 'faceclip')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model-human correlations for dataset subsets of Scaling models\n",
    "scaling_2b_df = model_human_df.copy().loc[[i for i in scaling_df.index.tolist() if '2B' in i]]\n",
    "scaling_400m_df = model_human_df.copy().loc[[i for i in scaling_df.index.tolist() if '400M' in i]]\n",
    "scaling_80m_df = model_human_df.copy().loc[[i for i in scaling_df.index.tolist() if '80M' in i]]\n",
    "scaling_2b_sub_df = model_human_df.copy().loc[[i for i in scaling_df.index.tolist() if '2B' in i and 'g-14' not in i and 'H-14' not in i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of attributes sorted from highest mean model-human similarity to lowest in the Scaling-2B models\n",
    "sorted_attributes = scaling_2b_df.mean(axis=0).sort_values(ascending=False).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each model family to its mean model-human similarity for each attribute\n",
    "family_mean_dict = {\n",
    "    'OpenAI': [],\n",
    "    'Scaling': [],\n",
    "    'Scaling 2B': [],\n",
    "    'Scaling 400M': [],\n",
    "    'Scaling 80M': [],\n",
    "    'Scaling 2B Sub': [],\n",
    "    'FaceCLIP': []\n",
    "}\n",
    "\n",
    "# Create a dictionary mapping each model family to its standard error of the mean model-human similarity for each attribute\n",
    "family_error_dict = {\n",
    "    'OpenAI': [],\n",
    "    'Scaling': [],\n",
    "    'Scaling 2B': [],\n",
    "    'Scaling 400M': [],\n",
    "    'Scaling 80M': [],\n",
    "    'Scaling 2B Sub': [],\n",
    "    'FaceCLIP': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean model-human similarity and standard error of the mean model-human similarity for each attribute in each model family\n",
    "for attribute in sorted_attributes:\n",
    "    family_mean_dict['OpenAI'].append(openai_df[attribute].mean())\n",
    "    family_mean_dict['Scaling'].append(scaling_df[attribute].mean())\n",
    "    family_mean_dict['Scaling 2B'].append(scaling_2b_df[attribute].mean())\n",
    "    family_mean_dict['Scaling 400M'].append(scaling_400m_df[attribute].mean())\n",
    "    family_mean_dict['Scaling 80M'].append(scaling_80m_df[attribute].mean())\n",
    "    family_mean_dict['Scaling 2B Sub'].append(scaling_2b_sub_df[attribute].mean())\n",
    "    family_mean_dict['FaceCLIP'].append(faceclip_df[attribute].mean())\n",
    "\n",
    "    family_error_dict['OpenAI'].append(openai_df[attribute].sem())\n",
    "    family_error_dict['Scaling'].append(scaling_df[attribute].sem())\n",
    "    family_error_dict['Scaling 2B'].append(scaling_2b_df[attribute].sem())\n",
    "    family_error_dict['Scaling 400M'].append(scaling_400m_df[attribute].sem())\n",
    "    family_error_dict['Scaling 80M'].append(scaling_80m_df[attribute].sem())\n",
    "    family_error_dict['Scaling 2B Sub'].append(scaling_2b_sub_df[attribute].sem())\n",
    "    family_error_dict['FaceCLIP'].append(faceclip_df[attribute].sem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LaTeX table string of the mean model-human similarity and standard error of the mean model-human similarity for each attribute in each model family\n",
    "tex_string = f'Attribute\\tOpenAI\\tOpenAI_Error\\tScaling\\tScaling_Error\\tScaling_2B\\tScaling_2B_Error\\tScaling_400M\\tScaling_400M_Error\\tScaling_80M\\tScaling_80M_Error\\tScaling_2B_Sub\\tScaling_2B_Sub_Error\\tFaceCLIP\\tFaceCLIP_Error\\n'\n",
    "\n",
    "# Iterate through each attribute sorted from highest mean model-human similarity to lowest in the Scaling-2B models\n",
    "for idx, attribute in enumerate(sorted_attributes):\n",
    "\n",
    "    # Add a row to the LaTeX table string for each attribute\n",
    "    tex_string += f'{attribute}\\t'\n",
    "\n",
    "    # Add the mean model-human similarity and standard error of the mean model-human similarity for each attribute in each model family to the LaTeX table string\n",
    "    for family in family_mean_dict.keys():\n",
    "        tex_string += f'{family_mean_dict[family][idx]}\\t{family_error_dict[family][idx]}\\t'\n",
    "\n",
    "    # Add a newline to the LaTeX table string after each attribute\n",
    "    tex_string += '\\n'\n",
    "\n",
    "# Print the string\n",
    "print(tex_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean correlation coefficient for each attribute across all models\n",
    "openai_mean = openai_df.mean(axis=0)\n",
    "scaling_mean = scaling_df.mean(axis=0)\n",
    "faceclip_mean = faceclip_df.mean(axis=0)\n",
    "\n",
    "# Create a dataframe of the mean correlation coefficients for each attribute across all models\n",
    "combined_mean_df = pd.concat([openai_mean, scaling_mean, faceclip_mean, human_irr], axis=1)\n",
    "combined_mean_df.columns = ['OpenAI', 'Scaling', 'FaceCLIP', 'Human']\n",
    "combined_mean_df['Human'] = combined_mean_df['Human'] / 100\n",
    "\n",
    "# Export combined_mean_df to data table for use in creating a Tikz scatterplot\n",
    "combined_mean_df.to_csv('combined_mean_df.csv', index=False, header=True, float_format='%.2f', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seaborn scatterplot of the mean correlation coefficient for each attribute plotted against the human inter-rater reliability for each attribute\n",
    "# Approximates the TikZ scatterplot created in the paper\n",
    "sns.scatterplot(data=combined_mean_df, x='Human', y='OpenAI', color='blue', label='OpenAI', alpha=0.5, s=20, marker='o', edgecolor='black')\n",
    "sns.scatterplot(data=combined_mean_df, x='Human', y='Scaling', color='red', label='Scaling', alpha=0.5, s=20, marker='o', edgecolor='black')\n",
    "sns.scatterplot(data=combined_mean_df, x='Human', y='FaceCLIP', color='green', label='FaceCLIP', alpha=0.5, s=20, marker='o', edgecolor='black')\n",
    "sns.regplot(data=combined_mean_df, x='Human', y='OpenAI', color='blue', scatter=False, label='OpenAI', ci=None, line_kws={'linestyle': '--', 'color': 'blue', 'linewidth': 1})\n",
    "sns.regplot(data=combined_mean_df, x='Human', y='Scaling', color='red', scatter=False, label='Scaling', ci=None, line_kws={'linestyle': '--', 'color': 'red', 'linewidth': 1})\n",
    "sns.regplot(data=combined_mean_df, x='Human', y='FaceCLIP', color='green', scatter=False, label='FaceCLIP', ci=None, line_kws={'linestyle': '--', 'color': 'green', 'linewidth': 1})\n",
    "plt.xlabel('Human Inter-Rater Reliability')\n",
    "plt.ylabel('Model-Human Correlation')\n",
    "plt.ylim(-.5,1)\n",
    "plt.xlim(.2,1)\n",
    "plt.legend()\n",
    "plt.title('Model-Human Correlation vs. Human Inter-Rater Reliability')\n",
    "\n",
    "# Format scatterplot\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show scatterplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix for mean model-human similarity for each attribute across all models, plus human inter-rater reliability\n",
    "corr_matrix = combined_mean_df.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlelogram of correlation matrix, approximating the TikZ correlelogram created in the paper\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='Blues', vmin=.4, vmax=1, center=.7, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot_kws={\"size\": 14, \"weight\": \"bold\"},\n",
    "            mask=np.triu(np.ones_like(corr_matrix, dtype=int))-np.eye(corr_matrix.shape[0], dtype=int), cbar=False)\n",
    "hfont = {'fontname':'Times New Roman', 'size': '14'}\n",
    "plt.yticks(rotation=90, **hfont)\n",
    "plt.xticks(rotation=0, **hfont)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
