{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir, path\n",
    "\n",
    "from utils import create_attribute_dict, create_model_association_df, create_model_human_similarity_dict\n",
    "from utils import cohens_d, paired_ttest, round_and_format, round_and_format_cohens_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path constants\n",
    "OMI_PATH = 'omi/attribute_means.csv'\n",
    "\n",
    "HUMAN_IRR_PATH = 'peterson_irr/human_irr.csv'\n",
    "ATTRIBUTE_PATH = 'prompts'\n",
    "MODEL_IMPRESSIONS_PATH = 'first_impression_similarities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in OMI attribute rating data\n",
    "omi_ratings = pd.read_csv(OMI_PATH, index_col=0)\n",
    "\n",
    "# Get a list of the 34 OMI attributes\n",
    "omi_attributes = omi_ratings.columns.to_list()\n",
    "\n",
    "# View the first 5 rows of the data\n",
    "omi_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each attribute to its positive polar prompt\n",
    "attribute_dict = create_attribute_dict(path.join(ATTRIBUTE_PATH,'attributes.txt'))\n",
    "\n",
    "# Create a dictionary mapping each attribute to its opposite prompt (the prompt for the opposing pole of the attribute)\n",
    "opposite_dict = create_attribute_dict(path.join(ATTRIBUTE_PATH,'attributes_opposites.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the models with similarity data saved as pickles\n",
    "model_pickles = [i for i in listdir(MODEL_IMPRESSIONS_PATH) if i.split('.')[-1] == 'pkl' and i.split('_')[0] == 'scaling']\n",
    "\n",
    "# Get a list of the model names from the pickle file names\n",
    "model_names = [i.split('_first_impression_similarities.pkl')[0] for i in model_pickles]\n",
    "\n",
    "# Create a dictionary mapping model names to their similarity data\n",
    "models_dict = dict(zip(model_names, model_pickles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store model-human correlation and statistical significance dataframes\n",
    "model_human_dfs, model_human_sig_dfs = [], []\n",
    "\n",
    "# Iterate through each model\n",
    "for model_name, model_file in models_dict.items():\n",
    "\n",
    "    # Read in the model similarity data\n",
    "    with open(path.join(MODEL_IMPRESSIONS_PATH, model_file), 'rb') as f:\n",
    "        model_similarity_dict = pkl.load(f)\n",
    "    \n",
    "    # Create a dataframe of the model similarity data\n",
    "    model_similarity_df = pd.DataFrame(model_similarity_dict)\n",
    "\n",
    "    # Create a dataframe of the difference between the cosine similarity of each image to the positive prompt and the negative prompt for each attribute in a model\n",
    "    model_association_df = create_model_association_df(model_similarity_df, attribute_dict, opposite_dict, baseline='difference')\n",
    "\n",
    "    # Create dictionaries of the Spearman's r correlation coefficient and significances between the model association and the OMI rating for each attribute in a model\n",
    "    model_human_correlations, model_human_sigs = create_model_human_similarity_dict(model_association_df, attribute_dict, omi_ratings)\n",
    "\n",
    "    # Create dataframes of the model-human correlations and significances\n",
    "    model_human_df, model_human_sig_df = pd.DataFrame(model_human_correlations, index=[model_name]), pd.DataFrame(model_human_sigs, index=[model_name])\n",
    "\n",
    "    # Append the model-human correlation and significance dataframes to lists\n",
    "    model_human_dfs.append(model_human_df)\n",
    "    model_human_sig_dfs.append(model_human_sig_df)\n",
    "\n",
    "# Concatenate model-human correlation and significance dataframes into single dataframes with model names as indices\n",
    "model_human_df = pd.concat(model_human_dfs)\n",
    "model_human_sig_df = pd.concat(model_human_sig_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model names by dataset size\n",
    "models_2b = [model for model in model_human_df.index if 'Data-2B' in model and 'Model-g-14' not in model and 'Model-H-14' not in model]\n",
    "models_400m = [model for model in model_human_df.index if 'Data-400M' in model]\n",
    "models_80m = [model for model in model_human_df.index if 'Data-80M' in model]\n",
    "\n",
    "# Check that the number of models in each dataset size is correct - 9 each\n",
    "len(models_2b), len(models_400m), len(models_80m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes of model-human correlations by dataset size\n",
    "df_2b = model_human_df.loc[models_2b]\n",
    "df_400m = model_human_df.loc[models_400m]\n",
    "df_80m = model_human_df.loc[models_80m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute means, standard deviations, and maximums for each attribute in each dataset size\n",
    "means_2b = [df_2b[attribute].mean() for attribute in omi_attributes]\n",
    "means_400m = [df_400m[attribute].mean() for attribute in omi_attributes]\n",
    "means_80m = [df_80m[attribute].mean() for attribute in omi_attributes]\n",
    "\n",
    "stds_2b = [df_2b[attribute].std() for attribute in omi_attributes]\n",
    "stds_400m = [df_400m[attribute].std() for attribute in omi_attributes]\n",
    "stds_80m = [df_80m[attribute].std() for attribute in omi_attributes]\n",
    "\n",
    "max_2b = [df_2b[attribute].max() for attribute in omi_attributes]\n",
    "max_400m = [df_400m[attribute].max() for attribute in omi_attributes]\n",
    "max_80m = [df_80m[attribute].max() for attribute in omi_attributes]\n",
    "\n",
    "# Compute Cohen's d and paired t-test p-values for each attribute in each dataset size\n",
    "d_2b_400m = [cohens_d(df_2b[attribute], df_400m[attribute]) for attribute in omi_attributes]\n",
    "d_2b_80m = [cohens_d(df_2b[attribute], df_80m[attribute]) for attribute in omi_attributes]\n",
    "d_400m_80m = [cohens_d(df_400m[attribute], df_80m[attribute]) for attribute in omi_attributes]\n",
    "\n",
    "t_2b_400m = [paired_ttest(df_2b[attribute], df_400m[attribute]).pvalue for attribute in omi_attributes]\n",
    "t_2b_80m = [paired_ttest(df_2b[attribute], df_80m[attribute]).pvalue for attribute in omi_attributes]\n",
    "t_400m_80m = [paired_ttest(df_400m[attribute], df_80m[attribute]).pvalue for attribute in omi_attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table of means + stds, max, and Cohen's d for each attribute in each dataset size\n",
    "latex_strings, index_means = [], []\n",
    "\n",
    "# Iterate through each attribute\n",
    "for i, attribute in enumerate(omi_attributes):\n",
    "        \n",
    "        # Add the attribute name to the LaTeX table string\n",
    "        latex_table_string = attribute + ' & '\n",
    "        \n",
    "        # Add the mean and standard deviation for each dataset size to the LaTeX table string\n",
    "        latex_table_string += f\"{round_and_format(means_2b[i])} ({stds_2b[i]:.2f}) & \"\n",
    "        latex_table_string += f\"{round_and_format(means_400m[i])} ({stds_400m[i]:.2f}) & \"\n",
    "        latex_table_string += f\"{round_and_format(means_80m[i])} ({stds_80m[i]:.2f}) & \"\n",
    "        \n",
    "        # Add the maximum for each dataset size to the LaTeX table string\n",
    "        latex_table_string += f\"{round_and_format(max_2b[i])} & \"\n",
    "        latex_table_string += f\"{round_and_format(max_400m[i])} & \"\n",
    "        latex_table_string += f\"{round_and_format(max_80m[i])} & \"\n",
    "        \n",
    "        # Add the Cohen's d for each dataset size to the LaTeX table string\n",
    "        latex_table_string += f\"{round_and_format_cohens_d(d_2b_80m[i], t_2b_80m[i])} & \"\n",
    "        latex_table_string += f\"{round_and_format_cohens_d(d_400m_80m[i], t_400m_80m[i])} &\"\n",
    "        latex_table_string += f\"{round_and_format_cohens_d(d_2b_400m[i], t_2b_400m[i])}  \\\\\\\\\"\n",
    "\n",
    "        # Add the LaTeX table string to the list of LaTeX table strings\n",
    "        latex_strings.append(latex_table_string)\n",
    "\n",
    "        # Add the mean for each dataset size to the list of means\n",
    "        index_means.append(means_2b[i])\n",
    "\n",
    "# Sort the LaTeX table strings by the mean of the 2B dataset\n",
    "latex_strings = [latex_strings[i] for i in reversed(np.argsort(index_means))]\n",
    "table_string = '\\n'.join(latex_strings)\n",
    "\n",
    "# Print the LaTeX table string\n",
    "print(table_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in human inter-rater reliability data provided in the body of Peterson et al. (2022)\n",
    "human_irr = pd.read_csv(HUMAN_IRR_PATH, index_col=0)\n",
    "transpose_irr = human_irr.transpose() / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort columns for barplot based on the mean of the 2B dataset\n",
    "column_ranks = df_2b.mean().sort_values().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust to overlay data in a barplot, such that the largest mean is in the back and the smallest in the front\n",
    "\n",
    "# Dataframe of 2B attributes that are shorter than 400M attributes\n",
    "df400_taller = df_2b.copy()[[column for column in column_ranks if abs(df_2b[column].mean()) < abs(df_400m[column].mean())]]\n",
    "for column in column_ranks:\n",
    "    if column not in df400_taller:\n",
    "        df400_taller[column] = 0\n",
    "\n",
    "# Dataframe of 400M attributes that are shorter than 80M attributes\n",
    "df80_taller = df_400m.copy()[[column for column in column_ranks if abs(df_400m[column].mean()) < abs(df_80m[column].mean())]]\n",
    "for column in column_ranks:\n",
    "    if column not in df80_taller:\n",
    "        df80_taller[column] = 0\n",
    "\n",
    "# Dataframe of 2B attributes that are shorter than 80M attributes\n",
    "df2b80_taller = df_2b.copy()[[column for column in column_ranks if abs(df_2b[column].mean()) < abs(df_80m[column].mean())]]\n",
    "for column in column_ranks:\n",
    "    if column not in df2b80_taller:\n",
    "        df2b80_taller[column] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot of model-human similarity by dataset size\n",
    "\n",
    "# Column ranks are based on the mean of the 2B dataset\n",
    "column_ranks = df_2b.mean().sort_values().index.tolist()\n",
    "\n",
    "# Human IRR barplot - would come first if uncommented, because it is always the largest bar\n",
    "#sns.barplot(data=transpose_irr, orient='v', errorbar=None, edgecolor='black', order=column_ranks, hue_order=column_ranks, color='lightyellow', dodge=False, label='Human IRR')\n",
    "\n",
    "# Overlay barplots of model-human similarity by dataset size\n",
    "sns.barplot(data=df_2b, orient='v', errorbar=None, edgecolor='black', order=column_ranks, hue_order=column_ranks, color='salmon', dodge=False, label='2b', hatch='xxx')\n",
    "sns.barplot(data=df_400m, orient='v', errorbar=None,  edgecolor='black', order=column_ranks, hue_order=column_ranks, color='skyblue', dodge=False, label='400m', hatch='---')\n",
    "sns.barplot(data=df400_taller, orient='v', errorbar=None,  edgecolor='black', order=column_ranks, hue_order=column_ranks, color='salmon', dodge=False, hatch='xxx')\n",
    "sns.barplot(data=df_80m, orient='v', errorbar=None,  edgecolor='black', order=column_ranks, hue_order=column_ranks, color='lightgreen', dodge=False, label='80m', hatch='///')\n",
    "sns.barplot(data=df80_taller, orient='v', errorbar=None,   edgecolor='black', order=column_ranks, hue_order=column_ranks, color='skyblue', dodge=False, hatch='---')\n",
    "sns.barplot(data=df2b80_taller, orient='v', errorbar=None, edgecolor='black',  order=column_ranks, hue_order=column_ranks, color='salmon', dodge=False, hatch='xxx')\n",
    "\n",
    "# Set plot parameters\n",
    "plt.legend(title='Training Dataset Size')\n",
    "plt.ylim(-.35,1)\n",
    "plt.xlim(-1,34)\n",
    "\n",
    "# Make it large\n",
    "plt.gcf().set_size_inches(14, 8)\n",
    "\n",
    "# Get the current axis out of the plot and set the font sizes and families for the legend, axes, and title\n",
    "ax = plt.gca()\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='16', fontname='Times New Roman') \n",
    "plt.setp(ax.get_legend().get_title(), fontsize='16', fontname='Times New Roman') \n",
    "plt.xticks(rotation=30, ha='right', fontname='Times New Roman', fontsize=15)\n",
    "plt.yticks(fontname='Times New Roman', fontsize=14)\n",
    "plt.xlabel('OMI Attribute', fontname='Times New Roman', fontsize=16)\n",
    "plt.ylabel('Mean Model-Human Similarity (Spearman\\'s r)', rotation=90, fontname='Times New Roman', fontsize=16)\n",
    "plt.title('Model-Human Similarity by Training Dataset Size', fontname='Times New Roman', fontsize=20)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f'model_human_similarity_dataset_size.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing, just horizontal formatted\n",
    "\n",
    "# Column ranks are based on the mean of the 2B dataset\n",
    "column_ranks = df_2b.mean().sort_values().index.tolist()\n",
    "\n",
    "# Human IRR barplot - would come first if uncommented, because it is always the largest bar\n",
    "#sns.barplot(data=transpose_irr, orient='v', errorbar=None, edgecolor='black', order=column_ranks, hue_order=column_ranks, color='lightyellow', dodge=False, label='Human IRR')\n",
    "\n",
    "# Overlay barplots of model-human similarity by dataset size\n",
    "sns.barplot(data=df_2b, orient='h', errorbar=None, edgecolor='black', order=column_ranks, hue_order=column_ranks, color='salmon', dodge=False, label='2b', hatch='xxx')\n",
    "sns.barplot(data=df_400m, orient='h', errorbar=None,  edgecolor='black', order=column_ranks, hue_order=column_ranks, color='skyblue', dodge=False, label='400m', hatch='---')\n",
    "sns.barplot(data=df400_taller, orient='h', errorbar=None,  edgecolor='black', order=column_ranks, hue_order=column_ranks, color='salmon', dodge=False, hatch='xxx')\n",
    "sns.barplot(data=df_80m, orient='h', errorbar=None,  edgecolor='black', order=column_ranks, hue_order=column_ranks, color='lightgreen', dodge=False, label='80m', hatch='///')\n",
    "sns.barplot(data=df80_taller, orient='h', errorbar=None,   edgecolor='black', order=column_ranks, hue_order=column_ranks, color='skyblue', dodge=False, hatch='---')\n",
    "sns.barplot(data=df2b80_taller, orient='h', errorbar=None, edgecolor='black',  order=column_ranks, hue_order=column_ranks, color='salmon', dodge=False, hatch='xxx')\n",
    "\n",
    "# Set plot parameters\n",
    "plt.legend(title='Training Dataset Size', loc='lower right')\n",
    "plt.xlim(-.35,1)\n",
    "plt.ylim(-1,34)\n",
    "\n",
    "# Make it large\n",
    "plt.gcf().set_size_inches(14, 8)\n",
    "\n",
    "# Get the current axis out of the plot and set the font sizes and families for the legend, axes, and title\n",
    "ax = plt.gca()\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='16', fontname='Times New Roman') \n",
    "plt.setp(ax.get_legend().get_title(), fontsize='16', fontname='Times New Roman') \n",
    "plt.xticks(rotation=30, ha='right', fontname='Times New Roman', fontsize=13)\n",
    "plt.yticks(fontname='Times New Roman', fontsize=14)\n",
    "plt.ylabel('OMI Attribute', fontname='Times New Roman', fontsize=16)\n",
    "plt.xlabel('Model-Human Similarity - Spearman\\'s r', fontname='Times New Roman', fontsize=16)\n",
    "plt.title('Model-Human Similarity by Training Dataset Size', fontname='Times New Roman', fontsize=20)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f'model_human_similarity_dataset_size.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
